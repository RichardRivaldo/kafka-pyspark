{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from datetime import datetime\n",
    "from json import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kafka and Spark Streaming\n",
    "KAFKA_TOPIC = \"social_media_topic\"\n",
    "BOOTSTRAP_SERVER = \"localhost:9092\"\n",
    "\n",
    "ssc = StreamingContext(sc, 1) #stream each one second\n",
    "ssc.checkpoint(\"./checkpoint\")\n",
    "lines = KafkaUtils.createDirectStream(ssc, [KAFKA_TOPIC],\n",
    "                                      {\"metadata.broker.list\": BOOTSTRAP_SERVER})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/streaming/util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/streaming/dstream.py\", line 173, in takeAndPrint\n    taken = rdd.take(num + 1)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/rdd.py\", line 1360, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/context.py\", line 1069, in runJob\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-3-63d2095bcd47>\", line 32, in map_elements\nKeyError: 'channelId'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-3-63d2095bcd47>\", line 32, in map_elements\nKeyError: 'channelId'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-63d2095bcd47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/streaming/util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/streaming/dstream.py\", line 173, in takeAndPrint\n    taken = rdd.take(num + 1)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/rdd.py\", line 1360, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/context.py\", line 1069, in runJob\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-3-63d2095bcd47>\", line 32, in map_elements\nKeyError: 'channelId'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/bigdata/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-3-63d2095bcd47>\", line 32, in map_elements\nKeyError: 'channelId'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:06\n",
      "-------------------------------------------\n",
      "(('instagram', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['KPJWLIDHKWSMATHMKVKK']})\n",
      "(('instagram', '2022-04-25T14:49:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['KPJWLIDHKWSMATHMKVKK']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:07\n",
      "-------------------------------------------\n",
      "(('instagram', '2022-04-25T14:44:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['KPJWLIDHKWSMATHMKVKK']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:08\n",
      "-------------------------------------------\n",
      "(('twitter', '2022-04-25T14:52:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VOKVKZZLKTARCHNQPVTC']})\n",
      "(('instagram', '2022-04-25T14:34:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VOKVKZZLKTARCHNQPVTC']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:13\n",
      "-------------------------------------------\n",
      "(('youtube', '2022-04-25T14:28:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['XCPEOFMAXFWGSUTMWSQY']})\n",
      "(('twitter', '2022-04-25T15:00:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['AUOCPICXBGGLILDOPLPZ']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['AUOCPICXBGGLILDOPLPZ']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['AUOCPICXBGGLILDOPLPZ']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['AUOCPICXBGGLILDOPLPZ']})\n",
      "(('instagram', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['AUOCPICXBGGLILDOPLPZ']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['AUOCPICXBGGLILDOPLPZ']})\n",
      "(('youtube', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['ZRPNZSSTIFRDSSHUTPLS']})\n",
      "(('instagram', '2022-04-25T15:01:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['TKANHRAWDYHUURQXGXFG']})\n",
      "(('instagram', '2022-04-25T14:42:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['TKANHRAWDYHUURQXGXFG']})\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:14\n",
      "-------------------------------------------\n",
      "(('instagram', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OQBTWKXXSUBRZTHSUYAI']})\n",
      "(('instagram', '2022-04-25T14:48:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OQBTWKXXSUBRZTHSUYAI']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OQBTWKXXSUBRZTHSUYAI']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OQBTWKXXSUBRZTHSUYAI']})\n",
      "(('twitter', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['BVQQZMKZUHLJSRXZTBPF']})\n",
      "(('twitter', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['LNCLQJFCYJJGTENPVNWI']})\n",
      "(('instagram', '2022-04-25T14:27:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['KYOSOJCIRKQEQFCJIIMP']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['FLCADYBTIXHOKJHSTGBK']})\n",
      "(('twitter', '2022-04-25T14:24:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['FLCADYBTIXHOKJHSTGBK']})\n",
      "(('twitter', '2022-04-25T14:47:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['FLCADYBTIXHOKJHSTGBK']})\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:15\n",
      "-------------------------------------------\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['XIPZSSAINNENWVNMAFLT']})\n",
      "(('instagram', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['FBSIBSQLQUPKJKKFSPEY']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['DDUJYTGUVWJGTCWGFWWI']})\n",
      "(('twitter', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['TLOPVYPINEBDBLQGCQAC']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:16\n",
      "-------------------------------------------\n",
      "(('instagram', '2022-04-25T14:56:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['TLOPVYPINEBDBLQGCQAC']})\n",
      "(('youtube', '2022-04-25T15:00:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['NNWKSLFYPSBYXNZEYAFQ']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['ARMGVSNDVAEBWUNQNIVA']})\n",
      "(('instagram', '2022-04-25T14:56:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['ARMGVSNDVAEBWUNQNIVA']})\n",
      "(('instagram', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['ARMGVSNDVAEBWUNQNIVA']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:20\n",
      "-------------------------------------------\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['HCVIZXPEHUMEPLXIYWYP']})\n",
      "(('instagram', '2022-04-25T14:40:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['HCVIZXPEHUMEPLXIYWYP']})\n",
      "(('facebook', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['JGOYHFFEKGWNBOOEXUFK']})\n",
      "(('instagram', '2022-04-25T14:27:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['RUEWEVHZEJZAXWBYTLEO']})\n",
      "(('instagram', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['RUEWEVHZEJZAXWBYTLEO']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['RUEWEVHZEJZAXWBYTLEO']})\n",
      "(('instagram', '2022-04-25T15:02:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['RUEWEVHZEJZAXWBYTLEO']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:21\n",
      "-------------------------------------------\n",
      "(('twitter', '2022-04-25T14:57:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['EMZMWBRXUNZZAVVSSGFJ']})\n",
      "(('youtube', '2022-04-25T14:31:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['ZCDMXODVVKUQQKORWDUI']})\n",
      "(('youtube', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['ZCDMXODVVKUQQKORWDUI']})\n",
      "(('instagram', '2022-04-25T14:45:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['ZCDMXODVVKUQQKORWDUI']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['ZCDMXODVVKUQQKORWDUI']})\n",
      "(('twitter', '2022-04-25T14:38:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['WRYTGTBHDEXMVJREPFRZ']})\n",
      "(('instagram', '2022-04-25T15:01:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['HCDOYWBHZQSFELSDEIEF']})\n",
      "(('facebook', '2022-04-25T14:52:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['HCDOYWBHZQSFELSDEIEF']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:22\n",
      "-------------------------------------------\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['HCDOYWBHZQSFELSDEIEF']})\n",
      "(('instagram', '2022-04-25T14:37:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['HCDOYWBHZQSFELSDEIEF']})\n",
      "(('instagram', '2022-04-25T15:02:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['HCDOYWBHZQSFELSDEIEF']})\n",
      "(('twitter', '2022-04-25T14:38:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['BAODGHYADCOKIRMIUNGE']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['YTKWWDFLVSZYJUSGIXXA']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['BHAISWFSERBCEXHHMJMJ']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['CPXKGWCQVWAQUCSALRXC']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:26\n",
      "-------------------------------------------\n",
      "(('instagram', '2022-04-25T14:33:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['CJSFTGIJHFMYVVXOAJAP']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['SPNCANKKXFDMLCPHCDON']})\n",
      "(('facebook', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['SPNCANKKXFDMLCPHCDON']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:28\n",
      "-------------------------------------------\n",
      "(('instagram', '2022-04-25T14:36:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VZEFKUWEYYXULEYJMYQS']})\n",
      "(('twitter', '2022-04-25T14:57:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VZEFKUWEYYXULEYJMYQS']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VZEFKUWEYYXULEYJMYQS']})\n",
      "(('instagram', '2022-04-25T14:52:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VZEFKUWEYYXULEYJMYQS']})\n",
      "(('instagram', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OCYSLHFBEBQCJDJLULDM']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OYOHCPPVKBUFLSRXDAZB']})\n",
      "(('instagram', '2022-04-25T14:26:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['SEGDWGENSJKDKEDRWPYM']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VUNJAYTNEEMSETILTNUF']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VUNJAYTNEEMSETILTNUF']})\n",
      "(('instagram', '2022-04-25T15:03:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VUNJAYTNEEMSETILTNUF']})\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:30\n",
      "-------------------------------------------\n",
      "(('instagram', '2022-04-25T14:49:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['ZGPVGXHFXMPQAYKZVVDP']})\n",
      "(('instagram', '2022-04-25T14:25:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['IRSDUTDHDVOUZZDGHGQX']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:31\n",
      "-------------------------------------------\n",
      "(('facebook', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['KZWZFELQFOZIDZMOABZV']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['GDTTXCOZMJBKDURVQIPK']})\n",
      "(('youtube', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['GDTTXCOZMJBKDURVQIPK']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:36\n",
      "-------------------------------------------\n",
      "(('facebook', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['GFRBXGBJPLOFAZXZGDRS']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['GFRBXGBJPLOFAZXZGDRS']})\n",
      "(('twitter', '2022-04-25T14:50:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['GFRBXGBJPLOFAZXZGDRS']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['GFRBXGBJPLOFAZXZGDRS']})\n",
      "(('facebook', '2022-04-25T14:56:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['GFRBXGBJPLOFAZXZGDRS']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VCCOMKJLINKLFHVHZICL']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OYHXMCPMXNBRJJEMVUGL']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OYHXMCPMXNBRJJEMVUGL']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['GBSBFGMEKLQTAAUNQTSR']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['LPHCRATYCMCKHZDGYVOM']})\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:38\n",
      "-------------------------------------------\n",
      "(('facebook', '2022-04-25T14:48:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['BAODGHYADCOKIRMIUNGE']})\n",
      "(('instagram', '2022-04-25T14:54:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['BAODGHYADCOKIRMIUNGE']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['BAODGHYADCOKIRMIUNGE']})\n",
      "(('twitter', '2022-04-25T15:02:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['BAODGHYADCOKIRMIUNGE']})\n",
      "(('instagram', '2022-04-25T14:43:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['EFIIAAJHUNUGVBZIIWPB']})\n",
      "(('twitter', '2022-04-25T14:56:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['DTUXTLFBWBTEGGYCLGDT']})\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['RGSIVGJGNSGNSBWXVAIC']})\n",
      "(('twitter', '2022-04-25T14:43:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['QTKXDUBFDQAGJLMPQNNK']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:40\n",
      "-------------------------------------------\n",
      "(('twitter', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['YHYUZHAYDLWPJLCGELCK']})\n",
      "(('instagram', '2022-04-25T14:36:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['UKCOWZHVWLFJYKBJSDED']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['UDMLSMCBKAUOJRROHPTF']})\n",
      "(('instagram', '2022-04-25T14:44:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['FBCAKGHUVNLATXQKNUFP']})\n",
      "(('youtube', '2022-04-25T14:37:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['FBCAKGHUVNLATXQKNUFP']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['FBCAKGHUVNLATXQKNUFP']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:43\n",
      "-------------------------------------------\n",
      "(('instagram', '2022-04-25T14:58:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['JHQRKHKEGDLSMDBRLAVX']})\n",
      "(('instagram', '2022-04-25T15:02:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['JHQRKHKEGDLSMDBRLAVX']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OSLHSXSEQLJXOLOCMLZZ']})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:44\n",
      "-------------------------------------------\n",
      "(('facebook', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OSLHSXSEQLJXOLOCMLZZ']})\n",
      "(('instagram', '2022-04-25T14:28:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OAEEDHCIYYGHNPCWTHYX']})\n",
      "(('instagram', '2022-04-25T14:58:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['JUCDOJCLNXEBAXWEYSPO']})\n",
      "(('youtube', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['CLYALEEECXAQDNJBQNSF']})\n",
      "(('instagram', '2022-04-25T14:58:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['EMZMWBRXUNZZAVVSSGFJ']})\n",
      "(('instagram', '2022-04-25T14:57:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['OBVTTEWUSUDARARHASHZ']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['LENDKOILMMQKBBCGUNWK']})\n",
      "(('instagram', '2022-04-25T14:41:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['BELLOECYIQEMDSWPNXST']})\n",
      "(('instagram', '2022-04-25T15:02:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['XYZGEPSJVXIPSRDGQKHG']})\n",
      "(('instagram', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['XYZGEPSJVXIPSRDGQKHG']})\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-25 15:04:46\n",
      "-------------------------------------------\n",
      "(('youtube', '2022-04-25T15:04:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['DESNZURQVPZCXCCIPRCP']})\n",
      "(('instagram', '2022-04-25T14:50:00'), {'user_count': 1, 'stream_count': 1, 'user_ids': ['VDOXVVOWJYDUYIRNFJGM']})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_data(lines, window=60, sliding=60):\n",
    "    def convert_timestamp(ts, socmed_type):\n",
    "        result = ts\n",
    "        if socmed_type == \"twitter\":\n",
    "            result = datetime.strptime(ts, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "        elif socmed_type == \"youtube\":\n",
    "            result = datetime.strptime(ts, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        elif socmed_type == \"instagram\":\n",
    "            result = datetime.fromtimestamp(int(result))\n",
    "        elif socmed_type == \"facebook\":\n",
    "            result = datetime.strptime(ts, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "        result = result.replace(second=0, microsecond=0)\n",
    "        result = datetime.strftime(result, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        return result\n",
    "    \n",
    "    def map_elements(line):\n",
    "        el = loads(line[1])\n",
    "        data = {}\n",
    "        socmed_type = el[\"crawler_target\"][\"specific_resource_type\"]\n",
    "        timestamp = None\n",
    "        \n",
    "        if socmed_type == \"twitter\":\n",
    "            timestamp = convert_timestamp(el[\"created_at\"], socmed_type)\n",
    "            data[\"user_count\"] = 1\n",
    "            data[\"stream_count\"] = 1\n",
    "            data[\"user_ids\"] = [el[\"user_id_str\"]]\n",
    "        elif socmed_type == \"youtube\":\n",
    "            timestamp = convert_timestamp(el[\"snippet\"][\"publishedAt\"], socmed_type)\n",
    "            data[\"user_count\"] = 1\n",
    "            data[\"stream_count\"] = 1\n",
    "            data[\"user_ids\"] = [el[\"snippet\"][\"channelId\"]]\n",
    "        elif socmed_type == \"instagram\":\n",
    "            timestamp = convert_timestamp(el[\"created_time\"], socmed_type)\n",
    "            data[\"user_count\"] = 1\n",
    "            data[\"stream_count\"] = 1\n",
    "            data[\"user_ids\"] = [el[\"user\"][\"id\"]]\n",
    "        elif socmed_type == \"facebook\":\n",
    "            timestamp = convert_timestamp(el[\"created_time\"], socmed_type)\n",
    "            data[\"user_count\"] = 1\n",
    "            data[\"stream_count\"] = 1\n",
    "            data[\"user_ids\"] = [el[\"from\"][\"id\"]]\n",
    "\n",
    "        return ((socmed_type, timestamp), data)\n",
    "    \n",
    "    # Streaming Main\n",
    "    process_result = lines.map(map_elements)\n",
    "    \n",
    "    return process_result\n",
    " \n",
    "social_media = calculate_data(lines)\n",
    "social_media.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
